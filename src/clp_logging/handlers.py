import logging
import os
import socket
import sys
import time
import warnings
from abc import ABCMeta, abstractmethod
from contextlib import nullcontext
from math import floor
from pathlib import Path
from queue import Empty, Queue
from signal import SIGINT, signal, SIGTERM
from threading import RLock, Thread, Timer
from types import FrameType
from typing import Any, Callable, ClassVar, Dict, List, IO, Optional, Tuple, Union


import tzlocal
from clp_ffi_py.ir import FourByteEncoder, Serializer
from clp_ffi_py.utils import serialize_dict_to_msgpack
from zstandard import FLUSH_FRAME, ZstdCompressionWriter, ZstdCompressor

import base64
import boto3
import botocore
import datetime
import hashlib
import io

from clp_logging.auto_generated_kv_pairs_utils import AutoGeneratedKeyValuePairsBuffer

from clp_logging.protocol import (
    BYTE_ORDER,
    EOF_CHAR,
    INT_MAX,
    INT_MIN,
    SIZEOF_INT,
    UINT_MAX,
    ULONG_MAX,
)
from clp_logging.utils import Timestamp

# TODO: lock writes to zstream if GIL ever goes away
# Note: no need to quote "Queue[Tuple[int, bytes]]" in python 3.9

DEFAULT_LOG_FORMAT: str = " %(levelname)s %(name)s %(message)s"
WARN_PREFIX: str = " [WARN][clp_logging]"
AUTO_GENERATED_KV_PAIRS_KEY: str = "auto_generated_kv_pairs"
USER_GENERATED_KV_PAIRS_KEY: str = "user_generated_kv_pairs"

# Define the multipart upload size limits
MIN_UPLOAD_PART_SIZE = 5 * 1024 * 1024  # 5 MB
MAX_UPLOAD_PART_SIZE = 5 * 1024 * 1024 * 1024  # 5 GB
MAX_PART_NUM_PER_UPLOAD = 10000


def _init_timeinfo(fmt: Optional[str], tz: Optional[str]) -> Tuple[str, str]:
    """
    Return the timestamp format and timezone (in TZID format) that should be
    used. If not specified by the user, this function will choose default values
    to use.

    The timestamp format (`fmt`) defaults to a format for the Java readers, due
    to compatibility issues between language time libraries.
    (`datatime.isoformat` is always used for the timestamp format in python
    readers.)

    The timezone format (`tz`) first defaults to the system local timezone.

    If that fails it will default to UTC. In the future sanitization
    of user input should also go here.

    :param fmt: Timestamp format written in preamble to be used when generating
    the logs with a reader.
    :param tz: Timezone in TZID format written in preamble to be used when
    generating the timestamp from Unix epoch time.
    :return: Tuple of timestamp format string and timezone in TZID format.
    """
    if not fmt:
        fmt = "yyyy-MM-d H:m:s.A"
    if not tz:
        try:
            tz = tzlocal.get_localzone_name()
        except Exception:
            tz = "UTC"

    return fmt, tz


def _encode_log_event(msg: str, last_timestamp_ms: int) -> Tuple[bytearray, int]:
    """
    Encodes the log event with the input log message and reference timestamp.

    :param msg: Input log message.
    :param last_timestamp_ms: The timestamp of the last log event. Will be used
        to calculate the timestamp delta.
    :return: A tuple of the encoded log event and the associated timestamp.
    """
    timestamp_ms: int = floor(time.time() * 1000)
    clp_msg: bytearray = FourByteEncoder.encode_message_and_timestamp_delta(
        timestamp_ms - last_timestamp_ms, msg.encode()
    )
    return clp_msg, timestamp_ms


class CLPBaseHandler(logging.Handler, metaclass=ABCMeta):
    def __init__(self) -> None:
        super().__init__()
        self.formatter: logging.Formatter = logging.Formatter(DEFAULT_LOG_FORMAT)

    # override
    def setFormatter(self, fmt: Optional[logging.Formatter]) -> None:
        """
        Check user `fmt` and remove any timestamp token to avoid double printing
        the timestamp.
        """
        if not fmt or not fmt._fmt:
            return

        fmt_str: str = fmt._fmt
        if "asctime" in fmt_str:
            found: Optional[str] = None
            if fmt_str.startswith("%(asctime)s "):
                found = "%(asctime)s"
            elif fmt_str.startswith("{asctime} "):
                found = "{asctime}"
            elif fmt_str.startswith("${asctime} "):
                found = "${asctime}"

            if found:
                fmt._fmt = fmt_str.replace(found, "")
                self._warn(f"replacing '{found}' with clp_logging timestamp format")
            else:
                fmt._fmt = DEFAULT_LOG_FORMAT
                self._warn(f"replacing '{fmt_str}' with '{DEFAULT_LOG_FORMAT}'")
        else:
            fmt._fmt = " " + fmt_str
            self._warn("prepending clp_logging timestamp to formatter")

        fmt._style = fmt._style.__class__(fmt._fmt)
        self.formatter = fmt

    def _warn(self, msg: str) -> None:
        self._write(logging.WARN, f"{WARN_PREFIX} {msg}\n")

    @abstractmethod
    def _write(self, loglevel: int, msg: str) -> None:
        raise NotImplementedError("_write must be implemented by derived handlers")

    # override
    def emit(self, record: logging.LogRecord) -> None:
        """
        Implements `logging.Handler.emit` to ensure
        `logging.Handler.handleError` is always called and so derived classes
        only need to implement `_write` instead of implementing this method.
        """
        msg: str = self.format(record) + "\n"
        try:
            self._write(record.levelno, msg)
        except Exception:
            self.handleError(record)


class CLPLogLevelTimeout:
    """
    A configurable timeout feature based on logging level that can be added to a
    CLPHandler. To schedule a timeout, two timers are calculated with each new
    log event. There is no distinction between the timer that triggers a timeout
    and once a timeout occurs both timers are reset. A timeout will always flush
    the zstandard frame and then call a user supplied function (`timeout_fn`).
    An additional timeout is always triggered on closing the logging handler.

    The two timers are implemented using `threading.Timer`. Each timer utilizes
    a map that associates each log level to a time delta (in milliseconds).
    Both the time deltas and the log levels are user configurable.
    The timers:
        - Hard timer: Represents the maximum elapsed time between generating a
          log event and triggering a timeout. The hard timer can only decrease
          towards a timeout occuring. It will decrease if a log level's time
          delta would schedule the timeout sooner. In other words, seeing a
          more important log level will cause a timeout to occur sooner. It is
          not possible for the hard timer to increase (a log whose level's
          delta would schedule a timeout after the current hard timer will not
          update the hard timer).
        - Soft timer: Represents the maximum elapsed time to wait for a new log
          event to be generated before triggering a timeout. Seeing a new log
          event will always cause the soft timer to be recalculated. The delta
          used to calculate the new soft timer is the lowest delta seen since
          the last timeout. Therefore, if we've seen a log level with a low
          delta, that delta will continue to be used to calculate the soft
          timer until a timeout occurs.

    Thread safety:
        - This class locks any operations on the stream set by `set_ostream`.
        - Any logging handler with a timeout object should lock the stream
          operations using the lock return by `get_lock()`.
    """

    # delta times in milliseconds
    # note: logging.FATAL == logging.CRITICAL and
    #       logging.WARN == logging.WARNING
    _HARD_TIMEOUT_DELTAS: Dict[int, int] = {
        logging.FATAL: 5 * 60 * 1000,
        logging.ERROR: 5 * 60 * 1000,
        logging.WARN: 10 * 60 * 1000,
        logging.INFO: 30 * 60 * 1000,
        logging.DEBUG: 30 * 60 * 1000,
    }
    _SOFT_TIMEOUT_DELTAS: Dict[int, int] = {
        logging.FATAL: 5 * 1000,
        logging.ERROR: 10 * 1000,
        logging.WARN: 15 * 1000,
        logging.INFO: 3 * 60 * 1000,
        logging.DEBUG: 3 * 60 * 1000,
    }

    def __init__(
        self,
        timeout_fn: Callable[[], None],
        hard_timeout_deltas: Dict[int, int] = _HARD_TIMEOUT_DELTAS,
        soft_timeout_deltas: Dict[int, int] = _SOFT_TIMEOUT_DELTAS,
    ) -> None:
        """
        Constructs a `CLPLogLevelTimeout` object which can be added to a
        CLPHandler to enable the timeout feature. `timeout_fn` is the only
        required parameter, but can be an empty function. Functionally, this
        will ensure a zstandard frame is flushed periodically. See
        `_HARD_TIMEOUT_DELTAS` and `_SOFT_TIMEOUT_DELTAS` for the default
        timeout values in milliseconds.

        :param timeout_fn: A user function that will be called when a timeout
            (hard or soft) occurs.
        :param hard_timeout_deltas: A dictionary, mapping a log level (as an
            int) to the maximum elapsed time (in milliseconds) between
            generating a log event and triggering a timeout.
        :param soft_timeout_deltas: A dictionary, mapping a log level (as an
            int) to the maximum elapsed time to wait (in milliseconds) for a new
            log event to be generated before triggering a timeout.
        """
        self.hard_timeout_deltas: Dict[int, int] = hard_timeout_deltas
        self.soft_timeout_deltas: Dict[int, int] = soft_timeout_deltas
        self.timeout_fn: Callable[[], None] = timeout_fn
        self.next_hard_timeout_ts: int = ULONG_MAX
        self.min_soft_timeout_delta: int = ULONG_MAX
        self.ostream: Optional[Union[ZstdCompressionWriter, IO[bytes]]] = None
        self.hard_timeout_thread: Optional[Timer] = None
        self.soft_timeout_thread: Optional[Timer] = None
        self.lock: RLock = RLock()

    def set_ostream(self, ostream: Union[ZstdCompressionWriter, IO[bytes]]) -> None:
        self.ostream = ostream

    def get_lock(self) -> RLock:
        return self.lock

    def timeout(self) -> None:
        """
        Wraps the call to the user supplied `timeout_fn` ensuring that any
        existing timeout threads are cancelled, `next_hard_timeout_ts` and
        `min_soft_timeout_delta` are reset, and the zstandard frame is flushed.
        """
        with self.get_lock():
            if self.hard_timeout_thread:
                self.hard_timeout_thread.cancel()
            if self.soft_timeout_thread:
                self.soft_timeout_thread.cancel()
            self.next_hard_timeout_ts = ULONG_MAX
            self.min_soft_timeout_delta = ULONG_MAX

            if self.ostream:
                if isinstance(self.ostream, ZstdCompressionWriter):
                    self.ostream.flush(FLUSH_FRAME)
                else:
                    self.ostream.flush()
            self.timeout_fn()

    def update(self, loglevel: int, log_timestamp_ms: int, log_fn: Callable[[str], None]) -> None:
        """
        Carries out the logic to schedule the next timeout based on the current
        log.

        :param loglevel: The log level (verbosity) of the current log.
        :param log_timestamp_ms: The timestamp in milliseconds of the current
            log.
        :param logfn: A function used for internal logging by the library. This
            allows us to correctly log through the handler itself rather than
            just printing to stdout/stderr.
        """
        hard_timeout_delta: int
        if loglevel not in self.hard_timeout_deltas:
            log_fn(
                f"{WARN_PREFIX} log level {loglevel} not in self.hard_timeout_deltas; defaulting"
                " to _HARD_TIMEOUT_DELTAS[logging.INFO].\n"
            )
            hard_timeout_delta = CLPLogLevelTimeout._HARD_TIMEOUT_DELTAS[logging.INFO]
        else:
            hard_timeout_delta = self.hard_timeout_deltas[loglevel]

        new_hard_timeout_ts: int = log_timestamp_ms + hard_timeout_delta
        if new_hard_timeout_ts < self.next_hard_timeout_ts:
            if self.hard_timeout_thread:
                self.hard_timeout_thread.cancel()
            self.hard_timeout_thread = Timer(new_hard_timeout_ts / 1000 - time.time(), self.timeout)
            self.hard_timeout_thread.setDaemon(True)
            self.hard_timeout_thread.start()
            self.next_hard_timeout_ts = new_hard_timeout_ts

        soft_timeout_delta: int
        if loglevel not in self.soft_timeout_deltas:
            log_fn(
                f"{WARN_PREFIX} log level {loglevel} not in self.soft_timeout_deltas; defaulting"
                " to _SOFT_TIMEOUT_DELTAS[logging.INFO].\n"
            )
            soft_timeout_delta = CLPLogLevelTimeout._SOFT_TIMEOUT_DELTAS[logging.INFO]
        else:
            soft_timeout_delta = self.soft_timeout_deltas[loglevel]

        if soft_timeout_delta < self.min_soft_timeout_delta:
            self.min_soft_timeout_delta = soft_timeout_delta

        new_soft_timeout_ms: int = log_timestamp_ms + soft_timeout_delta
        if self.soft_timeout_thread:
            self.soft_timeout_thread.cancel()
        self.soft_timeout_thread = Timer(new_soft_timeout_ms / 1000 - time.time(), self.timeout)
        self.soft_timeout_thread.setDaemon(True)
        self.soft_timeout_thread.start()


def _get_mutex_context_from_loglevel_timeout(loglevel_timeout: Optional[CLPLogLevelTimeout]) -> Any:
    """
    Gets a mutual exclusive context manager for IR stream access.

    NOTE: The return type should be `AbstractContextManager[Optional[bool]]`,
    but it is annotated as `Any` to satisfy the linter in Python 3.7 and 3.8,
    as `AbstractContextManager` was introduced in Python 3.9 (#18239).

    :param loglevel_timeout: An optional `CLPLogLevelTimeout` object.
    :return: A context manager that either provides the lock from
        `loglevel_timeout` or a `nullcontext` if `loglevel_timeout` is `None`.
    """
    return loglevel_timeout.get_lock() if loglevel_timeout else nullcontext()


class CLPSockListener:
    """
    Server that listens to a named Unix domain socket for `CLPSockHandler`
    instances writing CLP IR.

    Can be started by explicitly calling `CLPSockListener.fork` or by
    instantiating a `CLPSockHandler` with `create_listener=True`. Can be stopped
    by either sending the process SIGINT, SIGTERM, or `EOF_CHAR`.
    """

    _signaled: ClassVar[bool] = False

    @staticmethod
    def _try_bind(sock: socket.socket, sock_path: Path) -> int:
        """
        Tries to bind the given socket to the given path.

        Bind will fail if the socket file exists due to:
            a. Another listener currently exists and is running
                -> try to connect to it
            b. A listener existed in the past, but is now gone
                -> recovery unsupported
        :return: 0 on success or < 0 on failure. 0: bind succeeds, -1: connect
        succeeds, -2: nothing worked
        """
        try:
            sock.bind(str(sock_path))
            return 0
        except OSError:
            pass

        test_sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        if test_sock.connect_ex(str(sock_path)) == 0:
            test_sock.close()
            sock.close()
            return -1
        else:
            return -2

    @staticmethod
    def _exit_handler(signum: int, frame: Optional[FrameType]) -> None:
        _signaled = True

    @staticmethod
    def _handle_client(
        conn: socket.socket,
        log_queue: "Queue[Tuple[int, bytes]]",
    ) -> int:
        """
        Continuously reads from an individual `CLPSockHandler` and sends the
        received messages to the aggregator thread.

        :param conn: Client socket where encoded messages and their length are
        received.
        :param log_queue: Queue with `CLPSockListener._aggregator` thread to
        write encoded messages.
        """
        int_buf: bytearray = bytearray(SIZEOF_INT)
        while not CLPSockListener._signaled:
            try:
                read: int = conn.recv_into(int_buf, SIZEOF_INT)
                assert read == SIZEOF_INT
                size: int = int.from_bytes(int_buf, BYTE_ORDER)
                if size == 0:
                    log_queue.put((0, EOF_CHAR))
                    break
                read = conn.recv_into(int_buf, SIZEOF_INT)
                assert read == SIZEOF_INT
                loglevel: int = int.from_bytes(int_buf, BYTE_ORDER)
                buf: bytearray = bytearray(size)
                view: memoryview = memoryview(buf)
                i: int = 0
                while i < size:
                    read = conn.recv_into(view[i:], size - i)
                    if read == 0:
                        raise OSError("handler conn.recv_into returned 0 before finishing")
                    i += read
                log_queue.put((loglevel, buf))
            except socket.timeout:  # TODO replaced with TimeoutError in python 3.10
                pass
            except OSError:
                conn.close()
                raise
        conn.close()
        return 0

    @staticmethod
    def _aggregator(
        log_path: Path,
        log_queue: "Queue[Tuple[int, bytes]]",
        timestamp_format: Optional[str],
        timezone: Optional[str],
        timeout: int,
        enable_compression: bool,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> int:
        """
        Continuously receive encoded messages from
        `CLPSockListener._handle_client` threads and write them to a Zstandard
        stream.

        :param log_path: Path to log file and used to derive socket name.
        :param log_queue: Queue with `CLPSockListener._handle_client` threads
        to write encoded messages.
        :param timestamp_format: Timestamp format written in preamble to be
        used when generating the logs with a reader.
        :param timezone: Timezone written in preamble to be used when
        generating the timestamp from Unix epoch time.
        :param timeout: timeout in seconds to prevent `Queue.get` from never
        :param enable_compression: use the zstd compress if setting to True
        returning and not closing properly on signal/EOF_CHAR.
        :return: 0 on successful exit
        """
        cctx: ZstdCompressor = ZstdCompressor()
        timestamp_format, timezone = _init_timeinfo(timestamp_format, timezone)
        last_timestamp_ms: int = floor(time.time() * 1000)  # convert to ms and truncate

        with log_path.open("ab") as log:
            # Since the compression may be disabled, context manager is not used
            ostream: Union[ZstdCompressionWriter, IO[bytes]] = (
                cctx.stream_writer(log) if enable_compression else log
            )

            if loglevel_timeout:
                loglevel_timeout.set_ostream(ostream)

            def log_fn(msg: str) -> None:
                nonlocal last_timestamp_ms
                buf: bytearray
                buf, last_timestamp_ms = _encode_log_event(msg, last_timestamp_ms)
                ostream.write(buf)

            ostream.write(
                FourByteEncoder.encode_preamble(last_timestamp_ms, timestamp_format, timezone)
            )
            while not CLPSockListener._signaled:
                msg: bytes
                try:
                    loglevel, msg = log_queue.get(timeout=timeout)
                except Empty:
                    continue
                if msg == EOF_CHAR:
                    break
                buf: bytearray = bytearray(msg)
                timestamp_ms: int = floor(time.time() * 1000)
                timestamp_buf: bytearray = FourByteEncoder.encode_timestamp_delta(
                    timestamp_ms - last_timestamp_ms
                )
                last_timestamp_ms = timestamp_ms
                buf += timestamp_buf
                with _get_mutex_context_from_loglevel_timeout(loglevel_timeout):
                    if loglevel_timeout:
                        loglevel_timeout.update(loglevel, last_timestamp_ms, log_fn)
                    ostream.write(buf)
            if loglevel_timeout:
                loglevel_timeout.timeout()

            with _get_mutex_context_from_loglevel_timeout(loglevel_timeout):
                ostream.write(EOF_CHAR)

                if enable_compression:
                    # Since we are not using context manager, the ostream should be
                    # explicitly closed.
                    ostream.close()
        # tell _server to exit
        CLPSockListener._signaled = True
        return 0

    @staticmethod
    def _server(
        parent_fd: int,
        log_path: Path,
        sock_path: Path,
        timestamp_format: Optional[str],
        timezone: Optional[str],
        timeout: int,
        enable_compression: bool,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> int:
        """
        The `CLPSockListener` server function run in a new process. Writes 1
        byte back to parent process for synchronization.

        :param parent_fd: Used to communicate to parent `CLPSockHandler`
        process that `_try_bind` has finished.
        :param log_path: Path to log file.
        :param sock_path: Path to socket file.
        :param timestamp_format: Timestamp format written in preamble to be
        used when generating the logs with a reader.
        :param timezone: Timezone written in preamble to be used when
        generating the timestamp from Unix epoch time.
        :param timeout: timeout in seconds to prevent block operations from
        never returning and not closing properly on signal/EOF_CHAR
        :return: 0 on successful exit, -1 if `CLPSockListener._try_bind` fails
        """
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        log_queue: "Queue[Tuple[int, bytes]]" = Queue()
        ret: int = CLPSockListener._try_bind(sock, sock_path)
        sock.listen()
        os.write(parent_fd, b"\x00")
        if ret < 0:
            return ret

        signal(SIGINT, CLPSockListener._exit_handler)
        signal(SIGTERM, CLPSockListener._exit_handler)

        Thread(
            target=CLPSockListener._aggregator,
            args=(
                log_path,
                log_queue,
                timestamp_format,
                timezone,
                timeout,
                enable_compression,
                loglevel_timeout,
            ),
            daemon=False,
        ).start()

        while not CLPSockListener._signaled:
            conn: socket.socket
            addr: Tuple[str, int]
            try:
                conn, addr = sock.accept()
                conn.settimeout(timeout)
                Thread(
                    target=CLPSockListener._handle_client, args=(conn, log_queue), daemon=False
                ).start()
            except socket.timeout:
                pass
        sock.close()
        sock_path.unlink()
        return 0

    @staticmethod
    def fork(
        log_path: Path,
        timestamp_format: Optional[str],
        timezone: Optional[str],
        timeout: int,
        enable_compression: bool,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> int:
        """
        Fork a process running `CLPSockListener._server` and use `os.setsid()`
        to give it another session id (and process group id). The parent will
        not return until the forked listener has either bound the socket or
        finished trying to.

        :param log_path: Path to log file and used to derive socket name.
        :param timestamp_format: Timestamp format written in preamble to be used
            when generating the logs with a reader.
        :param timezone: Timezone written in preamble to be used when generating
            the timestamp from Unix epoch time.
        :param timeout: timeout in seconds to prevent block operations from
            never returning and not closing properly on signal/EOF_CHAR
        :return: child pid
        """
        sock_path: Path = log_path.with_suffix(".sock")
        rfd: int
        wfd: int
        rfd, wfd = os.pipe()
        pid: int = os.fork()
        if pid == 0:
            os.setsid()
            sys.exit(
                CLPSockListener._server(
                    wfd,
                    log_path,
                    sock_path,
                    timestamp_format,
                    timezone,
                    timeout,
                    enable_compression,
                    loglevel_timeout,
                )
            )
        else:
            os.read(rfd, 1)
        return pid


class CLPSockHandler(CLPBaseHandler):
    """
    Similar to `logging.Handler.SocketHandler`, but the log is written to the
    socket in CLP IR encoding rather than bytes.

    It is also simplified to only work with Unix domain sockets. The log is
    written to a socket named `log_path.with_suffix(".sock")`
    """

    def __init__(
        self,
        log_path: Path,
        create_listener: bool = False,
        timestamp_format: Optional[str] = None,
        timezone: Optional[str] = None,
        timeout: int = 2,
        enable_compression: bool = True,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> None:
        """
        Constructor method that optionally spawns a `CLPSockListener`.

        :param log_path: Path to log file written by `CLPSockListener` used to
        derive socket name.
        :param create_listener: If true and the handler could not connect to an
        existing listener, CLPSockListener.fork is used to try and spawn one.
        This is safe to be used by concurrent `CLPSockHandler` instances.
        :param timestamp_format: Timestamp format written in preamble to be
        used when generating the logs with a reader. (Only used when creating a
        listener.)
        :param timezone: Timezone written in preamble to be used when
        generating the timestamp from Unix epoch time. (Only used when creating
        a listener.)
        :param timeout: timeout in seconds to prevent blocking operations from
        never returning and not closing properly on signal/EOF_CHAR
        """
        super().__init__()
        self.sock_path: Path = log_path.with_suffix(".sock")
        self.closed: bool = False
        self.sock: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.listener_pid: int = 0

        if self.sock.connect_ex(str(self.sock_path)) != 0:
            if create_listener:
                self.listener_pid = CLPSockListener.fork(
                    log_path,
                    timestamp_format,
                    timezone,
                    timeout,
                    enable_compression,
                    loglevel_timeout,
                )

            # If we fail to connect again, the listener failed to resolve any
            # issues, so we raise an exception as there is nothing new to try
            try:
                self.sock.connect(str(self.sock_path))
            except OSError:
                self.sock.close()
                raise

    # override
    def _write(self, loglevel: int, msg: str) -> None:
        try:
            if self.closed:
                raise RuntimeError("Socket already closed")
            clp_msg: bytearray = FourByteEncoder.encode_message(msg.encode())
            size: int = len(clp_msg)
            if size > UINT_MAX:
                raise NotImplementedError(
                    "Encoded message longer than UINT_MAX currently unsupported"
                )
            loglevelb: bytes = loglevel.to_bytes(SIZEOF_INT, BYTE_ORDER)
            if loglevel > INT_MAX or loglevel < INT_MIN:
                raise NotImplementedError("Log level larger than signed INT currently unsupported")
            sizeb: bytes = size.to_bytes(SIZEOF_INT, BYTE_ORDER)
            self.sock.sendall(sizeb)
            self.sock.sendall(loglevelb)
            self.sock.sendall(clp_msg)
        except Exception as e:
            self.sock.close()
            raise e

    # override
    def handleError(self, record: logging.LogRecord) -> None:
        self.sock.close()
        logging.Handler.handleError(self, record)

    # override
    def close(self) -> None:
        self.acquire()
        try:
            self.sock.close()
            super().close()
        finally:
            self.release()
        self.closed = True

    def stop_listener(self) -> None:
        try:
            self.sock.send((0).to_bytes(SIZEOF_INT, BYTE_ORDER))
        except Exception:
            sock: socket.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
            sock.connect(str(self.sock_path))
            sock.send((0).to_bytes(SIZEOF_INT, BYTE_ORDER))
            sock.close()
        self.close()


class CLPStreamHandler(CLPBaseHandler):
    """
    Similar to `logging.StreamHandler`, but the log is written to `stream` in
    CLP IR encoding rather than bytes or a string.

    :param stream: Output stream of bytes to write CLP encoded log messages to
    """

    def init(self, stream: IO[bytes]) -> None:
        self.cctx: ZstdCompressor = ZstdCompressor()
        self.ostream: Union[ZstdCompressionWriter, IO[bytes]] = (
            self.cctx.stream_writer(stream) if self.enable_compression else stream
        )
        self.last_timestamp_ms: int = floor(time.time() * 1000)  # convert to ms and truncate
        self.ostream.write(
            FourByteEncoder.encode_preamble(
                self.last_timestamp_ms, self.timestamp_format, self.timezone
            )
        )

    def __init__(
        self,
        stream: Optional[IO[bytes]],
        enable_compression: bool = True,
        timestamp_format: Optional[str] = None,
        timezone: Optional[str] = None,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> None:
        super().__init__()
        self.closed: bool = False
        self.enable_compression: bool = enable_compression
        if stream is None:
            stream = sys.stderr.buffer
        self.stream: IO[bytes] = stream
        self.timestamp_format: str
        self.timezone: str
        self.timestamp_format, self.timezone = _init_timeinfo(timestamp_format, timezone)
        self.init(self.stream)

        if loglevel_timeout:
            loglevel_timeout.set_ostream(self.ostream)
        self.loglevel_timeout = loglevel_timeout

    def _direct_write(self, msg: str) -> None:
        if self.closed:
            raise RuntimeError("Stream already closed")
        clp_msg: bytearray
        clp_msg, self.last_timestamp_ms = _encode_log_event(msg, self.last_timestamp_ms)
        with _get_mutex_context_from_loglevel_timeout(self.loglevel_timeout):
            self.ostream.write(clp_msg)

    # override
    def _write(self, loglevel: int, msg: str) -> None:
        if self.closed:
            raise RuntimeError("Stream already closed")
        clp_msg: bytearray
        clp_msg, self.last_timestamp_ms = _encode_log_event(msg, self.last_timestamp_ms)
        with _get_mutex_context_from_loglevel_timeout(self.loglevel_timeout):
            if self.loglevel_timeout:
                self.loglevel_timeout.update(loglevel, self.last_timestamp_ms, self._direct_write)
            self.ostream.write(clp_msg)

    # Added to logging.StreamHandler in python 3.7
    # override
    def setStream(self, stream: IO[bytes]) -> Optional[IO[bytes]]:
        if not self.stream:
            self.stream = stream
            self.init(stream)
            return None
        elif stream is self.stream:
            return None
        else:
            self.acquire()
            try:
                self.close()
                self.stream = stream
                self.init(stream)
            finally:
                self.release()
            return self.stream

    # override
    def close(self) -> None:
        if self.loglevel_timeout:
            self.loglevel_timeout.timeout()
        with _get_mutex_context_from_loglevel_timeout(self.loglevel_timeout):
            self.ostream.write(EOF_CHAR)
            self.ostream.close()
        self.closed = True
        super().close()


class CLPFileHandler(CLPStreamHandler):
    """
    Wrapper class that calls `open` for convenience.
    """

    def __init__(
        self,
        fpath: Path,
        mode: str = "ab",
        enable_compression: bool = True,
        timestamp_format: Optional[str] = None,
        timezone: Optional[str] = None,
        loglevel_timeout: Optional[CLPLogLevelTimeout] = None,
    ) -> None:
        self.fpath: Path = fpath
        super().__init__(
            open(fpath, mode), enable_compression, timestamp_format, timezone, loglevel_timeout
        )


class ClpKeyValuePairStreamHandler(logging.Handler):
    """
    A custom logging handler that serializes key-value pair log events into the
    CLP key-value pair IR format.

    Differences from `logging.StreamHandler`:

    - Log events (`logging.LogRecord`) should contain the key-value pairs that a user wants to log
      as a Python dictionary.
      - As a result, the key-value pairs will not be formatted into a string before being written.
    - The key-value pairs will be serialized into the CLP key-value pair IR format before writing to
      the stream.

    Key-value pairs in the log event must abide by the following rules:
    - Keys must be of type `str`.
    - Values must be one of the following types:
      - Primitives: `int`, `float`, `str`, `bool`, or `None`.
      - Arrays, where each array:
        - may contain primitive values, dictionaries, or nested arrays.
        - can be empty.
      - Dictionaries, where each dictionary:
        - must adhere to the aforementioned rules for keys and values.
        - can be empty.

    :param stream: A writable byte output stream to which the handler will write the serialized IR
        byte sequences.
    :param enable_compression: Whether to compress the serialized IR byte sequences using Zstandard.

    """

    def __init__(
        self,

        stream: IO[bytes],
        enable_compression: bool = True,
    ) -> None:
        super().__init__()

        self._enable_compression: bool = enable_compression
        self._serializer: Optional[Serializer] = None
        self._formatter: Optional[logging.Formatter] = None
        self._ostream: IO[bytes] = stream

        self._auto_gen_kv_pairs_buf: AutoGeneratedKeyValuePairsBuffer = (
            AutoGeneratedKeyValuePairsBuffer()
        )

        self._init_new_serializer(stream)

    # override
    def setFormatter(self, fmt: Optional[logging.Formatter]) -> None:
        if fmt is None:
            return
        warnings.warn(
            f"{self.__class__.__name__} doesn't currently support Formatters",
            category=RuntimeWarning,
        )
        self._formatter = fmt

    # override
    def emit(self, record: logging.LogRecord) -> None:
        """
        Implements `logging.Handler.emit` to encode the given record into CLP's
        IR format before it's written to the underlying stream.

        :param record: The log event to serialize.
        """
        try:
            self._write(record)
        except Exception:
            self.handleError(record)

    # override
    def setStream(self, stream: IO[bytes]) -> Optional[IO[bytes]]:
        """
        Sets the instance's stream to the given value, if it's different from
        the current value. The old stream is flushed before the new stream is
        set.

        NOTE: The old stream will also be closed by this method.

        :param stream: A writable byte output stream to which the handler will write the serialized
            IR byte sequences.
        :return: The old stream if the stream was changed, or `None` if it wasn't.
        """

        # NOTE: This function is implemented by mirroring CPython's implementation.

        if stream is self._ostream:
            return None

        old_stream: IO[bytes] = self._ostream
        with self.lock if self.lock else nullcontext():
            # TODO: The following call will close the old stream whereas `logging.StreamHandler`'s
            # implementation will only flush the stream without closing it. To support
            # `logging.StreamHandler`'s behaviour, we need `clp_ffi_py.ir.Serializer` to allow
            # closing the serializer without closing the underlying output stream.
            self._init_new_serializer(stream)
            self._ostream = stream
        return old_stream

    # override
    def close(self) -> None:
        if self._is_closed():
            return
        self._close_serializer()
        super().close()

    def _is_closed(self) -> bool:
        return self._serializer is None

    def _close_serializer(self) -> None:
        """
        Closes the current serializer if it's open.

        NOTE: The underlying output stream will also be closed.
        """
        if self._is_closed():
            return
        assert self._serializer is not None
        self._serializer.close()
        self._serializer = None

    def _init_new_serializer(self, stream: IO[bytes]) -> None:
        """
        Initializes a new serializer that will write to the given stream.

        :param stream: The stream that the underlying serializer will write to.
        """
        self._close_serializer()
        self._serializer = Serializer(
            ZstdCompressor().stream_writer(stream) if self._enable_compression else stream
        )

    def _write(self, record: logging.LogRecord) -> None:
        """
        Writes the log event into the underlying serializer.

        :param record: The log event to serialize.
        :raise RuntimeError: If the handler has been already closed.
        :raise TypeError: If `record.msg` is not a Python dictionary.
        """
        if self._is_closed():
            raise RuntimeError("Stream already closed.")

        if not isinstance(record.msg, dict):
            raise TypeError("`record.msg` must be a Python dictionary.")

        self._serialize_kv_pair_log_event(
            self._auto_gen_kv_pairs_buf.generate(Timestamp.now(), record), record.msg
        )

    def _serialize_kv_pair_log_event(
        self, auto_gen_kv_pairs: Dict[str, Any], user_gen_kv_pairs: Dict[str, Any]
    ) -> None:
        """
        :param auto_gen_kv_pairs: A dict of auto-generated kv-pairs.
        :param user_gen_kv_pairs: A dict of user-generated kv-pairs.
        """
        if self._is_closed():
            raise RuntimeError("Stream already closed.")
        assert self._serializer is not None
        self._serializer.serialize_log_event_from_msgpack_map(
            serialize_dict_to_msgpack(auto_gen_kv_pairs),
            serialize_dict_to_msgpack(user_gen_kv_pairs),
        )


class CLPS3Handler(CLPBaseHandler):
    """
    Log is written to stream in CLP IR encoding, and uploaded to s3_bucket

    :param s3_bucket: S3 bucket to upload CLP encoded log messages to
    :param stream: Target stream to write log messages to
    :param enable_compression: Option to enable/disable stream compression
        Default: True
    :param timestamp_format: Timestamp format written in preamble to be
        used when generating the logs with a reader.
    :param timezone: Timezone written in preamble to be used when
        generating the timestamp from Unix epoch time.
    :param aws_access_key_id: User's public access key for the S3 bucket.
    :param aws_secret_access_key: User's private access key for the S3 bucket.
    :param s3_directory: S3 remote directory to upload objects to.
    :param use_multipart_upload: Option to use multipart upload to upload
        stream segments or use PutObject to upload the entire buffer.
        Default: True
    :param max_part_num: Maximum number of parts allowed for a multipart upload
        session before uploading to a new object. Default: 10000
    :param upload_part_size: Maximum size of a part in a multipart upload
        session before writing to a new part. Default: 5MB
    """

    def __init__(
        self,
        s3_bucket: str,
        stream: Optional[IO[bytes]] = None,
        enable_compression: bool = True,
        timestamp_format: Optional[str] = None,
        timezone: Optional[str] = None,
        aws_access_key_id: Optional[str] = None,
        aws_secret_access_key: Optional[str] = None,
        s3_directory: Optional[str] = None,
        use_multipart_upload: Optional[bool] = True,
        max_part_num: Optional[int] = None,
        upload_part_size: Optional[int] = MIN_UPLOAD_PART_SIZE
    ) -> None:
        super().__init__()
        self.closed: bool = False
        self.enable_compression: bool = enable_compression
        self._local_buffer: io.BytesIO = io.BytesIO()
        if stream is None:
            stream = self._local_buffer
        self._ostream: IO[bytes] = stream
        self.timestamp_format: str
        self.timezone: str
        self.timestamp_format, self.timezone = _init_timeinfo(timestamp_format, timezone)
        self._init_stream(stream)

        # Configure s3-related variables
        self.s3_bucket: str = s3_bucket
        try:
            self._s3_client = boto3.client(
                "s3",
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key
            ) if aws_access_key_id and aws_secret_access_key else boto3.client("s3")
        except botocore.exceptions.NoCredentialsError:
            raise RuntimeError("AWS credentials not found. Please configure your credentials.")
        except botocore.exceptions.ClientError as e:
            raise RuntimeError(f"Failed to initialize AWS client: {e}")
        self._remote_folder_path: Optional[str] = None
        self._remote_file_count: int = 1
        self._start_timestamp: datetime = datetime.datetime.now()
        self.s3_directory: str = (s3_directory.rstrip('/') + '/') if s3_directory else ''
        self._obj_key: str = self._remote_log_naming()

        self.use_multipart_upload = use_multipart_upload
        if self.use_multipart_upload:
            # Configure size limit of a part in multipart upload
            self.upload_part_size: int
            if MIN_UPLOAD_PART_SIZE <= upload_part_size <= MAX_UPLOAD_PART_SIZE:
                self.upload_part_size = upload_part_size
            else:
                raise RuntimeError(
                    f"Invalid upload_part_size: {upload_part_size}. "
                    f"It must be between {MIN_UPLOAD_PART_SIZE} and {MAX_UPLOAD_PART_SIZE}."
                )
            self.max_part_num: int = max_part_num if max_part_num else MAX_PART_NUM_PER_UPLOAD
            self._uploaded_parts: List[Dict[str, int | str]] = []
            self._upload_index: int = 1
            create_ret: Dict[str, Any] = self._s3_client.create_multipart_upload(
                Bucket=self.s3_bucket, Key=self._obj_key, ChecksumAlgorithm="SHA256"
            )
            self._upload_id: int = create_ret["UploadId"]
            if not self._upload_id or not isinstance(self._upload_id, str):
                raise RuntimeError("Failed to obtain a valid Upload ID from S3.")

    def _init_stream(self, stream: IO[bytes]) -> None:
        """
        Initialize and configure output stream

        :param stream: Target stream to write log messages to
        """
        self.cctx: ZstdCompressor = ZstdCompressor()
        self._ostream: Union[ZstdCompressionWriter, IO[bytes]] = (
            self.cctx.stream_writer(self._local_buffer) if self.enable_compression else stream
        )
        self.last_timestamp_ms: int = floor(time.time() * 1000)  # convert to ms and truncate
        self._ostream.write(
            FourByteEncoder.encode_preamble(
                self.last_timestamp_ms, self.timestamp_format, self.timezone
            )
        )

    def _remote_log_naming(self) -> str:
        """
        Set the name of the target S3 object key to upload to
        """
        self._remote_folder_path: str = f"{self.s3_directory}{self._start_timestamp.year}/{self._start_timestamp.month}/{self._start_timestamp.day}"

        new_filename: str
        upload_time: str = str(int(self._start_timestamp.timestamp()))

        file_count: str = f"-{self._remote_file_count}"

        # Compression uses zstd format
        if self.enable_compression:
            new_filename = f"{self._remote_folder_path}/{upload_time}_log{file_count}.clp.zst"
        else:
            new_filename = f"{self._remote_folder_path}/{upload_time}_log{file_count}.clp"
        return new_filename

    # override
    def _write(self, loglevel: int, msg: str) -> None:
        """
        Write the log message stream into a local buffer.
        (With use_multipart_upload) Update the part number if the local buffer
        exceeds a predetermined buffer size. Then clear the local buffer.
        """
        if self.closed:
            raise RuntimeError("Stream already closed")
        clp_msg: bytearray
        clp_msg, self.last_timestamp_ms = _encode_log_event(msg, self.last_timestamp_ms)

        # Write log stream to a local buffer and flush to upload
        self._ostream.write(clp_msg)
        if not self.use_multipart_upload:
            self._ostream.write(EOF_CHAR)
        self._flush()

        if self.use_multipart_upload and self._local_buffer.tell() >= self.upload_part_size:
            # Rotate after maximum number of parts
            if self._upload_index >= self.max_part_num:
                self._complete_multipart_upload()
                self._ostream.close()
                self._local_buffer = io.BytesIO()
                self._init_stream(self._local_buffer)
                self._remote_file_count += 1
                self._obj_key = self._remote_log_naming()
                self._uploaded_parts = []
                self._upload_index = 1
                create_ret = self._s3_client.create_multipart_upload(
                    Bucket=self.s3_bucket, Key=self._obj_key, ChecksumAlgorithm="SHA256"
                )
                self._upload_id = create_ret["UploadId"]
                if not self._upload_id:
                    raise RuntimeError("Failed to initialize new upload ID.")
            else:
                self._upload_index += 1
                self._local_buffer.seek(0)
                self._local_buffer.truncate(0)


    def _flush(self) -> None:
        """
        Upload local buffer to the S3 bucket using upload_part if
        use_multipart_upload = True, otherwise use put_object.
        """
        self._ostream.flush()
        data: bytes = self._local_buffer.getvalue()
        sha256_checksum: str = base64.b64encode(hashlib.sha256(data).digest()).decode('utf-8')

        if self.use_multipart_upload:
            try:
                response: Dict[str, Any] = self._s3_client.upload_part(
                    Bucket=self.s3_bucket,
                    Key=self._obj_key,
                    Body=data,
                    PartNumber=self._upload_index,
                    UploadId=self._upload_id,
                    ChecksumSHA256=sha256_checksum,
                )

                # Verify integrity of the uploaded part using SHA256 Checksum
                if response["ChecksumSHA256"] != sha256_checksum:
                    raise ValueError(f"Checksum mismatch for part {self._upload_index}. Upload aborted.")

                # Store both ETag and SHA256 for validation
                upload_status: Dict[str, int | str] = {
                    "PartNumber": self._upload_index,
                    "ETag": response["ETag"],
                    "ChecksumSHA256": response["ChecksumSHA256"],
                }

                # Determine the part to which the new upload_status belongs
                if len(self._uploaded_parts) > self._upload_index - 1:
                    self._uploaded_parts[self._upload_index-1] = upload_status
                else:
                    self._uploaded_parts.append(upload_status)

            except Exception as e:
                self._s3_client.abort_multipart_upload(
                    Bucket=self.s3_bucket, Key=self._obj_key, UploadId=self._upload_id
                )
                raise Exception(
                    f'Multipart Upload on Part {self._upload_index}: {e}'
                ) from e
        else:
            self._ostream.write(EOF_CHAR)
            try:
                self._s3_client.put_object(
                    Bucket=self.s3_bucket,
                    Key=self._obj_key,
                    Body=data,
                    ContentEncoding='zstd' if self.enable_compression else 'binary',
                    ChecksumSHA256=sha256_checksum
                )

                # Verify integrity of the upload using SHA256 Checksum
                response: Dict[str, Any] = self._s3_client.head_object(
                    Bucket=self.s3_bucket,
                    Key=self._obj_key
                )
                if 'ChecksumSHA256' in response:
                    s3_checksum: str = response['ChecksumSHA256']
                    if s3_checksum != sha256_checksum:
                        raise ValueError(f"Checksum mismatch. Upload aborted.")

            except Exception as e:
                raise Exception(f'Failed to upload using PutObject: {e}')

    def _complete_multipart_upload(self) -> None:
        """
        Complete a multipart upload session and clear the local buffer.
        """
        # Flush EOF marker to the local buffer and upload
        self._ostream.write(EOF_CHAR)
        self._flush()
        self._local_buffer.seek(0)
        self._local_buffer.truncate(0)

        try:
            self._s3_client.complete_multipart_upload(
                Bucket=self.s3_bucket,
                Key=self._obj_key,
                UploadId=self._upload_id,
                MultipartUpload={
                    "Parts": [
                        {
                            "PartNumber": part["PartNumber"],
                            "ETag": part["ETag"],
                            "ChecksumSHA256": part["ChecksumSHA256"],
                        }
                        for part in self._uploaded_parts
                    ]
                },
            )
        except Exception as e:
            self._s3_client.abort_multipart_upload(
                Bucket=self.s3_bucket, Key=self._obj_key, UploadId=self._upload_id
            )
            raise Exception(
                f'Multipart Upload on Part {self._upload_index}: {e}'
            ) from e

    # override
    def close(self) -> None:
        """
        Complete the upload if needed. Close the stream and the handler.
        """
        if self.use_multipart_upload:
            self._complete_multipart_upload()
        self._ostream.close()
        self.closed = True
        super().close()
